{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6a0d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.8.20 (default, Oct  3 2024, 15:24:27) \n",
      "[GCC 11.2.0]\n",
      "torch: 1.12.1\n",
      "fvcore: 0.1.5.post20221221\n",
      "basicsr: 1.3.4.9\n",
      "cuda available: True\n",
      "cuda: 11.3\n",
      "device: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import fvcore\n",
    "import basicsr\n",
    "import sys\n",
    "\n",
    "print(\"python:\", sys.version)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"fvcore:\", fvcore.__version__)\n",
    "print(\"basicsr:\", basicsr.__version__ if hasattr(basicsr, \"__version__\") else \"unknown\")\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda:\", torch.version.cuda)\n",
    "print(\"device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b418f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vclab2080ti/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/vclab2080ti/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> patch_size = 1 img_size = 64 patches_resolution = [64, 64]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from drct.archs.DrctPlusCA_arch import DRCTPLUSCA   # ✅ 여기!\n",
    "\n",
    "\n",
    "model = DRCTPLUSCA(\n",
    "    img_size=64,\n",
    "    patch_size=1,\n",
    "    in_chans=3,\n",
    "    embed_dim=180,\n",
    "    depths=(6, 6, 6, 6,6,6),\n",
    "    num_heads=(6, 6, 6, 6,6,6),\n",
    "    window_size=16,\n",
    "    compress_ratio=3,\n",
    "    squeeze_factor=30,\n",
    "    conv_scale=0.01,\n",
    "    overlap_ratio=0.5,\n",
    "    mlp_ratio=2.,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=0.,\n",
    "    attn_drop_rate=0.,\n",
    "    drop_path_rate=0.1,\n",
    "    norm_layer=nn.LayerNorm,\n",
    "    ape=False,\n",
    "    patch_norm=True,\n",
    "    use_checkpoint=False,\n",
    "    upscale=4,\n",
    "    img_range=1.,\n",
    "    upsampler='pixelshuffle',\n",
    "    resi_connection='1conv',\n",
    "    gc=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb816800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name                             | #elements or shape   |\n",
      "|:---------------------------------|:---------------------|\n",
      "| model                            | 25.3M                |\n",
      "|  conv_first                      |  5.0K                |\n",
      "|   conv_first.weight              |   (180, 3, 3, 3)     |\n",
      "|   conv_first.bias                |   (180,)             |\n",
      "|  patch_embed                     |  0.4K                |\n",
      "|   patch_embed.norm               |   0.4K               |\n",
      "|    patch_embed.norm.weight       |    (180,)            |\n",
      "|    patch_embed.norm.bias         |    (180,)            |\n",
      "|  layers                          |  24.6M               |\n",
      "|   layers.0                       |   4.1M               |\n",
      "|    layers.0.swin1                |    0.5M              |\n",
      "|    layers.0.adjust1              |    5.8K              |\n",
      "|    layers.0.swin2                |    0.6M              |\n",
      "|    layers.0.adjust2              |    6.8K              |\n",
      "|    layers.0.swin3                |    0.8M              |\n",
      "|    layers.0.adjust3              |    7.8K              |\n",
      "|    layers.0.swin4                |    0.9M              |\n",
      "|    layers.0.adjust4              |    8.9K              |\n",
      "|    layers.0.swin5                |    1.1M              |\n",
      "|    layers.0.adjust5              |    55.6K             |\n",
      "|   layers.1                       |   4.1M               |\n",
      "|    layers.1.swin1                |    0.5M              |\n",
      "|    layers.1.adjust1              |    5.8K              |\n",
      "|    layers.1.swin2                |    0.6M              |\n",
      "|    layers.1.adjust2              |    6.8K              |\n",
      "|    layers.1.swin3                |    0.8M              |\n",
      "|    layers.1.adjust3              |    7.8K              |\n",
      "|    layers.1.swin4                |    0.9M              |\n",
      "|    layers.1.adjust4              |    8.9K              |\n",
      "|    layers.1.swin5                |    1.1M              |\n",
      "|    layers.1.adjust5              |    55.6K             |\n",
      "|   layers.2                       |   4.1M               |\n",
      "|    layers.2.swin1                |    0.5M              |\n",
      "|    layers.2.adjust1              |    5.8K              |\n",
      "|    layers.2.swin2                |    0.6M              |\n",
      "|    layers.2.adjust2              |    6.8K              |\n",
      "|    layers.2.swin3                |    0.8M              |\n",
      "|    layers.2.adjust3              |    7.8K              |\n",
      "|    layers.2.swin4                |    0.9M              |\n",
      "|    layers.2.adjust4              |    8.9K              |\n",
      "|    layers.2.swin5                |    1.1M              |\n",
      "|    layers.2.adjust5              |    55.6K             |\n",
      "|   layers.3                       |   4.1M               |\n",
      "|    layers.3.swin1                |    0.5M              |\n",
      "|    layers.3.adjust1              |    5.8K              |\n",
      "|    layers.3.swin2                |    0.6M              |\n",
      "|    layers.3.adjust2              |    6.8K              |\n",
      "|    layers.3.swin3                |    0.8M              |\n",
      "|    layers.3.adjust3              |    7.8K              |\n",
      "|    layers.3.swin4                |    0.9M              |\n",
      "|    layers.3.adjust4              |    8.9K              |\n",
      "|    layers.3.swin5                |    1.1M              |\n",
      "|    layers.3.adjust5              |    55.6K             |\n",
      "|   layers.4                       |   4.1M               |\n",
      "|    layers.4.swin1                |    0.5M              |\n",
      "|    layers.4.adjust1              |    5.8K              |\n",
      "|    layers.4.swin2                |    0.6M              |\n",
      "|    layers.4.adjust2              |    6.8K              |\n",
      "|    layers.4.swin3                |    0.8M              |\n",
      "|    layers.4.adjust3              |    7.8K              |\n",
      "|    layers.4.swin4                |    0.9M              |\n",
      "|    layers.4.adjust4              |    8.9K              |\n",
      "|    layers.4.swin5                |    1.1M              |\n",
      "|    layers.4.adjust5              |    55.6K             |\n",
      "|   layers.5                       |   4.1M               |\n",
      "|    layers.5.swin1                |    0.5M              |\n",
      "|    layers.5.adjust1              |    5.8K              |\n",
      "|    layers.5.swin2                |    0.6M              |\n",
      "|    layers.5.adjust2              |    6.8K              |\n",
      "|    layers.5.swin3                |    0.8M              |\n",
      "|    layers.5.adjust3              |    7.8K              |\n",
      "|    layers.5.swin4                |    0.9M              |\n",
      "|    layers.5.adjust4              |    8.9K              |\n",
      "|    layers.5.swin5                |    1.1M              |\n",
      "|    layers.5.adjust5              |    55.6K             |\n",
      "|  norm                            |  0.4K                |\n",
      "|   norm.weight                    |   (180,)             |\n",
      "|   norm.bias                      |   (180,)             |\n",
      "|  conv_after_body                 |  0.3M                |\n",
      "|   conv_after_body.weight         |   (180, 180, 3, 3)   |\n",
      "|   conv_after_body.bias           |   (180,)             |\n",
      "|  conv_before_upsample            |  0.1M                |\n",
      "|   conv_before_upsample.0         |   0.1M               |\n",
      "|    conv_before_upsample.0.weight |    (64, 180, 3, 3)   |\n",
      "|    conv_before_upsample.0.bias   |    (64,)             |\n",
      "|  upsample                        |  0.3M                |\n",
      "|   upsample.0                     |   0.1M               |\n",
      "|    upsample.0.weight             |    (256, 64, 3, 3)   |\n",
      "|    upsample.0.bias               |    (256,)            |\n",
      "|   upsample.2                     |   0.1M               |\n",
      "|    upsample.2.weight             |    (256, 64, 3, 3)   |\n",
      "|    upsample.2.bias               |    (256,)            |\n",
      "|  conv_last                       |  1.7K                |\n",
      "|   conv_last.weight               |   (3, 64, 3, 3)      |\n",
      "|   conv_last.bias                 |   (3,)               |\n",
      "Params: 25316745\n",
      "Forward Passes: 1\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "# ✅ 모델 입력(더미 입력) 크기 정의\n",
    "input_tensor = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# ✅ FLOPs(부동소수점 연산량) 계산\n",
    "# flop_count = FlopCountAnalysis(model, input_tensor)\n",
    "# flops = flop_count.total()\n",
    "\n",
    "# ✅ 파라미터 수 계산\n",
    "# - parameter_count_table(model): 레이어별 파라미터 수를 보기 좋게 표 형태로 출력\n",
    "# - params: 전체 파라미터 수(직접 합산)\n",
    "params_table = parameter_count_table(model)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# ✅ Multi-Adds(MACs) 및 forward 횟수 설정\n",
    "# - 관례적으로 1 MAC ≈ 2 FLOPs(곱 1 + 덧셈 1)로 보는 경우가 많아서 FLOPs/2로 계산\n",
    "# multi_adds = flops / 2\n",
    "forward_passes = 1  # 더미 입력으로 forward를 몇 번 계산했는지(여기서는 1번)\n",
    "\n",
    "# ✅ 결과 출력\n",
    "print(params_table)\n",
    "print(f\"Params: {params}\")\n",
    "# print(f\"FLOPs: {flops}\")\n",
    "# print(f\"Multi-Adds: {multi_adds}\")\n",
    "print(f\"Forward Passes: {forward_passes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e55a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> patch_size = 1 img_size = 64 patches_resolution = [64, 64]\n",
      "model ok\n"
     ]
    }
   ],
   "source": [
    "import torch, os, sys\n",
    "from torch import nn\n",
    "\n",
    "from drct.archs.DRCT_arch import DRCT  # ✅ 여기만 바뀜\n",
    "\n",
    "model = DRCT(\n",
    "    img_size=64,\n",
    "    patch_size=1,\n",
    "    in_chans=3,\n",
    "    embed_dim=180,\n",
    "    depths=(6, 6, 6, 6,6,6),\n",
    "    num_heads=(6, 6, 6, 6,6,6),\n",
    "    window_size=16,\n",
    "    compress_ratio=3,\n",
    "    squeeze_factor=30,\n",
    "    conv_scale=0.01,\n",
    "    overlap_ratio=0.5,\n",
    "    mlp_ratio=2.,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=0.,\n",
    "    attn_drop_rate=0.,\n",
    "    drop_path_rate=0.1,\n",
    "    norm_layer=nn.LayerNorm,\n",
    "    ape=False,\n",
    "    patch_norm=True,\n",
    "    use_checkpoint=False,\n",
    "    upscale=4,\n",
    "    img_range=1.,\n",
    "    upsampler='pixelshuffle',\n",
    "    resi_connection='1conv',\n",
    "    gc=32\n",
    ")\n",
    "\n",
    "print(\"model ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486ee40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name                             | #elements or shape   |\n",
      "|:---------------------------------|:---------------------|\n",
      "| model                            | 14.1M                |\n",
      "|  conv_first                      |  5.0K                |\n",
      "|   conv_first.weight              |   (180, 3, 3, 3)     |\n",
      "|   conv_first.bias                |   (180,)             |\n",
      "|  patch_embed                     |  0.4K                |\n",
      "|   patch_embed.norm               |   0.4K               |\n",
      "|    patch_embed.norm.weight       |    (180,)            |\n",
      "|    patch_embed.norm.bias         |    (180,)            |\n",
      "|  layers                          |  13.4M               |\n",
      "|   layers.0                       |   2.2M               |\n",
      "|    layers.0.swin1                |    0.3M              |\n",
      "|    layers.0.adjust1              |    5.8K              |\n",
      "|    layers.0.swin2                |    0.4M              |\n",
      "|    layers.0.adjust2              |    6.8K              |\n",
      "|    layers.0.swin3                |    0.5M              |\n",
      "|    layers.0.adjust3              |    7.8K              |\n",
      "|    layers.0.swin4                |    0.5M              |\n",
      "|    layers.0.adjust4              |    8.9K              |\n",
      "|    layers.0.swin5                |    0.6M              |\n",
      "|    layers.0.adjust5              |    55.6K             |\n",
      "|   layers.1                       |   2.2M               |\n",
      "|    layers.1.swin1                |    0.3M              |\n",
      "|    layers.1.adjust1              |    5.8K              |\n",
      "|    layers.1.swin2                |    0.4M              |\n",
      "|    layers.1.adjust2              |    6.8K              |\n",
      "|    layers.1.swin3                |    0.5M              |\n",
      "|    layers.1.adjust3              |    7.8K              |\n",
      "|    layers.1.swin4                |    0.5M              |\n",
      "|    layers.1.adjust4              |    8.9K              |\n",
      "|    layers.1.swin5                |    0.6M              |\n",
      "|    layers.1.adjust5              |    55.6K             |\n",
      "|   layers.2                       |   2.2M               |\n",
      "|    layers.2.swin1                |    0.3M              |\n",
      "|    layers.2.adjust1              |    5.8K              |\n",
      "|    layers.2.swin2                |    0.4M              |\n",
      "|    layers.2.adjust2              |    6.8K              |\n",
      "|    layers.2.swin3                |    0.5M              |\n",
      "|    layers.2.adjust3              |    7.8K              |\n",
      "|    layers.2.swin4                |    0.5M              |\n",
      "|    layers.2.adjust4              |    8.9K              |\n",
      "|    layers.2.swin5                |    0.6M              |\n",
      "|    layers.2.adjust5              |    55.6K             |\n",
      "|   layers.3                       |   2.2M               |\n",
      "|    layers.3.swin1                |    0.3M              |\n",
      "|    layers.3.adjust1              |    5.8K              |\n",
      "|    layers.3.swin2                |    0.4M              |\n",
      "|    layers.3.adjust2              |    6.8K              |\n",
      "|    layers.3.swin3                |    0.5M              |\n",
      "|    layers.3.adjust3              |    7.8K              |\n",
      "|    layers.3.swin4                |    0.5M              |\n",
      "|    layers.3.adjust4              |    8.9K              |\n",
      "|    layers.3.swin5                |    0.6M              |\n",
      "|    layers.3.adjust5              |    55.6K             |\n",
      "|   layers.4                       |   2.2M               |\n",
      "|    layers.4.swin1                |    0.3M              |\n",
      "|    layers.4.adjust1              |    5.8K              |\n",
      "|    layers.4.swin2                |    0.4M              |\n",
      "|    layers.4.adjust2              |    6.8K              |\n",
      "|    layers.4.swin3                |    0.5M              |\n",
      "|    layers.4.adjust3              |    7.8K              |\n",
      "|    layers.4.swin4                |    0.5M              |\n",
      "|    layers.4.adjust4              |    8.9K              |\n",
      "|    layers.4.swin5                |    0.6M              |\n",
      "|    layers.4.adjust5              |    55.6K             |\n",
      "|   layers.5                       |   2.2M               |\n",
      "|    layers.5.swin1                |    0.3M              |\n",
      "|    layers.5.adjust1              |    5.8K              |\n",
      "|    layers.5.swin2                |    0.4M              |\n",
      "|    layers.5.adjust2              |    6.8K              |\n",
      "|    layers.5.swin3                |    0.5M              |\n",
      "|    layers.5.adjust3              |    7.8K              |\n",
      "|    layers.5.swin4                |    0.5M              |\n",
      "|    layers.5.adjust4              |    8.9K              |\n",
      "|    layers.5.swin5                |    0.6M              |\n",
      "|    layers.5.adjust5              |    55.6K             |\n",
      "|  norm                            |  0.4K                |\n",
      "|   norm.weight                    |   (180,)             |\n",
      "|   norm.bias                      |   (180,)             |\n",
      "|  conv_after_body                 |  0.3M                |\n",
      "|   conv_after_body.weight         |   (180, 180, 3, 3)   |\n",
      "|   conv_after_body.bias           |   (180,)             |\n",
      "|  conv_before_upsample            |  0.1M                |\n",
      "|   conv_before_upsample.0         |   0.1M               |\n",
      "|    conv_before_upsample.0.weight |    (64, 180, 3, 3)   |\n",
      "|    conv_before_upsample.0.bias   |    (64,)             |\n",
      "|  upsample                        |  0.3M                |\n",
      "|   upsample.0                     |   0.1M               |\n",
      "|    upsample.0.weight             |    (256, 64, 3, 3)   |\n",
      "|    upsample.0.bias               |    (256,)            |\n",
      "|   upsample.2                     |   0.1M               |\n",
      "|    upsample.2.weight             |    (256, 64, 3, 3)   |\n",
      "|    upsample.2.bias               |    (256,)            |\n",
      "|  conv_last                       |  1.7K                |\n",
      "|   conv_last.weight               |   (3, 64, 3, 3)      |\n",
      "|   conv_last.bias                 |   (3,)               |\n",
      "Params: 14139579\n",
      "Forward Passes: 1\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "# ✅ 모델 입력(더미 입력) 크기 정의\n",
    "input_tensor = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# ✅ FLOPs(부동소수점 연산량) 계산\n",
    "# flop_count = FlopCountAnalysis(model, input_tensor)\n",
    "# flops = flop_count.total()\n",
    "\n",
    "# ✅ 파라미터 수 계산\n",
    "# - parameter_count_table(model): 레이어별 파라미터 수를 보기 좋게 표 형태로 출력\n",
    "# - params: 전체 파라미터 수(직접 합산)\n",
    "params_table = parameter_count_table(model)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# ✅ Multi-Adds(MACs) 및 forward 횟수 설정\n",
    "# - 관례적으로 1 MAC ≈ 2 FLOPs(곱 1 + 덧셈 1)로 보는 경우가 많아서 FLOPs/2로 계산\n",
    "# multi_adds = flops / 2\n",
    "forward_passes = 1  # 더미 입력으로 forward를 몇 번 계산했는지(여기서는 1번)\n",
    "\n",
    "# ✅ 결과 출력\n",
    "print(params_table)\n",
    "print(f\"Params: {params}\")\n",
    "# print(f\"FLOPs: {flops}\")\n",
    "# print(f\"Multi-Adds: {multi_adds}\")\n",
    "print(f\"Forward Passes: {forward_passes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60edc225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> x_size: (tensor(64), tensor(64)) token L: tensor(4096) expected(H*W): tensor(4096)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[180], expected input with shape [*, 180], but got input of size[1, 180, 4096]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9811aca08260>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# ✅ 먼저 total()을 호출해야 분석 결과가 생성됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mflops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflop_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# ✅ 이제부터 unsupported/uncalled 조회 가능\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py\u001b[0m in \u001b[0;36mtotal\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mint\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0maggregated\u001b[0m \u001b[0mstatistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mmodule_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_module_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mtotal_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_trace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"no_tracer_warning\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTracerWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_scoped_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aliases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# Assures even modules not in the trace graph are initialized to zero count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py\u001b[0m in \u001b[0;36m_get_scoped_trace_graph\u001b[0;34m(module, inputs, aliases)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mregister_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhook_handles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mONNXTracedModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         graph, out = torch._C._create_graph_by_tracing(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0min_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/projects/DRCT/drct/archs/DRCT_arch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsampler\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pixelshuffle'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0;31m# for classical SR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_after_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_before_upsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/projects/DRCT/drct/archs/DRCT_arch.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute_pos_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/projects/DRCT/drct/archs/DRCT_arch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 结构为 [B, num_patches, C]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 归一化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    190\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drct-jupyter/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2501\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m         )\n\u001b[0;32m-> 2503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[180], expected input with shape [*, 180], but got input of size[1, 180, 4096]"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "# ✅ (추천) FLOPs는 CPU에서: trace OOM/이상동작 줄어듦\n",
    "model_eval = copy.deepcopy(model).cpu().eval()\n",
    "input_tensor = torch.randn(1, 3, 64, 64)  # CPU tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    flop_count = FlopCountAnalysis(model_eval, input_tensor)\n",
    "\n",
    "    # ✅ 먼저 total()을 호출해야 분석 결과가 생성됨\n",
    "    flops = flop_count.total()\n",
    "\n",
    "    # ✅ 이제부터 unsupported/uncalled 조회 가능\n",
    "    print(\"unsupported_ops:\", flop_count.unsupported_ops())\n",
    "    print(\"uncalled_modules:\", flop_count.uncalled_modules())\n",
    "    print(\"FLOPs total:\", flops)\n",
    "\n",
    "# ✅ 파라미터 수 (CPU 모델 기준)\n",
    "params_table = parameter_count_table(model_eval)\n",
    "params = sum(p.numel() for p in model_eval.parameters())\n",
    "\n",
    "# ✅ Multi-Adds(MACs)\n",
    "multi_adds = flops / 2\n",
    "forward_passes = 1\n",
    "\n",
    "# ✅ 결과 출력\n",
    "print(params_table)\n",
    "print(f\"Params: {params}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Multi-Adds: {multi_adds}\")\n",
    "print(f\"Forward Passes: {forward_passes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "127482d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name                             | #elements or shape   |\n",
      "|:---------------------------------|:---------------------|\n",
      "| model                            | 4.4M                 |\n",
      "|  conv_first                      |  2.7K                |\n",
      "|   conv_first.weight              |   (96, 3, 3, 3)      |\n",
      "|   conv_first.bias                |   (96,)              |\n",
      "|  patch_embed                     |  0.2K                |\n",
      "|   patch_embed.norm               |   0.2K               |\n",
      "|    patch_embed.norm.weight       |    (96,)             |\n",
      "|    patch_embed.norm.bias         |    (96,)             |\n",
      "|  layers                          |  3.9M                |\n",
      "|   layers.0                       |   1.0M               |\n",
      "|    layers.0.swin1                |    76.1K             |\n",
      "|    layers.0.adjust1              |    3.1K              |\n",
      "|    layers.0.swin2                |    0.1M              |\n",
      "|    layers.0.adjust2              |    4.1K              |\n",
      "|    layers.0.swin3                |    0.2M              |\n",
      "|    layers.0.adjust3              |    5.2K              |\n",
      "|    layers.0.swin4                |    0.2M              |\n",
      "|    layers.0.adjust4              |    6.2K              |\n",
      "|    layers.0.swin5                |    0.3M              |\n",
      "|    layers.0.adjust5              |    21.6K             |\n",
      "|   layers.1                       |   1.0M               |\n",
      "|    layers.1.swin1                |    76.1K             |\n",
      "|    layers.1.adjust1              |    3.1K              |\n",
      "|    layers.1.swin2                |    0.1M              |\n",
      "|    layers.1.adjust2              |    4.1K              |\n",
      "|    layers.1.swin3                |    0.2M              |\n",
      "|    layers.1.adjust3              |    5.2K              |\n",
      "|    layers.1.swin4                |    0.2M              |\n",
      "|    layers.1.adjust4              |    6.2K              |\n",
      "|    layers.1.swin5                |    0.3M              |\n",
      "|    layers.1.adjust5              |    21.6K             |\n",
      "|   layers.2                       |   1.0M               |\n",
      "|    layers.2.swin1                |    76.1K             |\n",
      "|    layers.2.adjust1              |    3.1K              |\n",
      "|    layers.2.swin2                |    0.1M              |\n",
      "|    layers.2.adjust2              |    4.1K              |\n",
      "|    layers.2.swin3                |    0.2M              |\n",
      "|    layers.2.adjust3              |    5.2K              |\n",
      "|    layers.2.swin4                |    0.2M              |\n",
      "|    layers.2.adjust4              |    6.2K              |\n",
      "|    layers.2.swin5                |    0.3M              |\n",
      "|    layers.2.adjust5              |    21.6K             |\n",
      "|   layers.3                       |   1.0M               |\n",
      "|    layers.3.swin1                |    76.1K             |\n",
      "|    layers.3.adjust1              |    3.1K              |\n",
      "|    layers.3.swin2                |    0.1M              |\n",
      "|    layers.3.adjust2              |    4.1K              |\n",
      "|    layers.3.swin3                |    0.2M              |\n",
      "|    layers.3.adjust3              |    5.2K              |\n",
      "|    layers.3.swin4                |    0.2M              |\n",
      "|    layers.3.adjust4              |    6.2K              |\n",
      "|    layers.3.swin5                |    0.3M              |\n",
      "|    layers.3.adjust5              |    21.6K             |\n",
      "|  norm                            |  0.2K                |\n",
      "|   norm.weight                    |   (96,)              |\n",
      "|   norm.bias                      |   (96,)              |\n",
      "|  conv_after_body                 |  83.0K               |\n",
      "|   conv_after_body.weight         |   (96, 96, 3, 3)     |\n",
      "|   conv_after_body.bias           |   (96,)              |\n",
      "|  conv_before_upsample            |  55.4K               |\n",
      "|   conv_before_upsample.0         |   55.4K              |\n",
      "|    conv_before_upsample.0.weight |    (64, 96, 3, 3)    |\n",
      "|    conv_before_upsample.0.bias   |    (64,)             |\n",
      "|  upsample                        |  0.3M                |\n",
      "|   upsample.0                     |   0.1M               |\n",
      "|    upsample.0.weight             |    (256, 64, 3, 3)   |\n",
      "|    upsample.0.bias               |    (256,)            |\n",
      "|   upsample.2                     |   0.1M               |\n",
      "|    upsample.2.weight             |    (256, 64, 3, 3)   |\n",
      "|    upsample.2.bias               |    (256,)            |\n",
      "|  conv_last                       |  1.7K                |\n",
      "|   conv_last.weight               |   (3, 64, 3, 3)      |\n",
      "|   conv_last.bias                 |   (3,)               |\n",
      "Params: 4379963\n",
      "Forward Passes: 1\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "# ✅ 모델 입력(더미 입력) 크기 정의\n",
    "input_tensor = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# ✅ FLOPs(부동소수점 연산량) 계산\n",
    "# flop_count = FlopCountAnalysis(model, input_tensor)\n",
    "# flops = flop_count.total()\n",
    "\n",
    "# ✅ 파라미터 수 계산\n",
    "# - parameter_count_table(model): 레이어별 파라미터 수를 보기 좋게 표 형태로 출력\n",
    "# - params: 전체 파라미터 수(직접 합산)\n",
    "params_table = parameter_count_table(model)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# ✅ Multi-Adds(MACs) 및 forward 횟수 설정\n",
    "# - 관례적으로 1 MAC ≈ 2 FLOPs(곱 1 + 덧셈 1)로 보는 경우가 많아서 FLOPs/2로 계산\n",
    "# multi_adds = flops / 2\n",
    "forward_passes = 1  # 더미 입력으로 forward를 몇 번 계산했는지(여기서는 1번)\n",
    "\n",
    "# ✅ 결과 출력\n",
    "print(params_table)\n",
    "print(f\"Params: {params}\")\n",
    "# print(f\"FLOPs: {flops}\")\n",
    "# print(f\"Multi-Adds: {multi_adds}\")\n",
    "print(f\"Forward Passes: {forward_passes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "522cf545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 14,139,579\n",
      "                  module  num_params  percent\n",
      "         conv_after_body      291780     2.06\n",
      " layers.3.swin5.attn.qkv      285516     2.02\n",
      " layers.0.swin5.attn.qkv      285516     2.02\n",
      " layers.5.swin5.attn.qkv      285516     2.02\n",
      " layers.1.swin5.attn.qkv      285516     2.02\n",
      " layers.4.swin5.attn.qkv      285516     2.02\n",
      " layers.2.swin5.attn.qkv      285516     2.02\n",
      " layers.1.swin4.attn.qkv      229356     1.62\n",
      " layers.2.swin4.attn.qkv      229356     1.62\n",
      " layers.3.swin4.attn.qkv      229356     1.62\n",
      " layers.4.swin4.attn.qkv      229356     1.62\n",
      " layers.5.swin4.attn.qkv      229356     1.62\n",
      " layers.0.swin4.attn.qkv      229356     1.62\n",
      " layers.0.swin3.attn.qkv      179340     1.27\n",
      " layers.3.swin3.attn.qkv      179340     1.27\n",
      " layers.4.swin3.attn.qkv      179340     1.27\n",
      " layers.2.swin3.attn.qkv      179340     1.27\n",
      " layers.5.swin3.attn.qkv      179340     1.27\n",
      " layers.1.swin3.attn.qkv      179340     1.27\n",
      "              upsample.0      147712     1.04\n",
      "              upsample.2      147712     1.04\n",
      " layers.1.swin2.attn.qkv      135468     0.96\n",
      " layers.4.swin2.attn.qkv      135468     0.96\n",
      " layers.2.swin2.attn.qkv      135468     0.96\n",
      " layers.5.swin2.attn.qkv      135468     0.96\n",
      " layers.3.swin2.attn.qkv      135468     0.96\n",
      " layers.0.swin2.attn.qkv      135468     0.96\n",
      "  layers.2.swin3.mlp.fc1      119560     0.85\n",
      "  layers.0.swin3.mlp.fc1      119560     0.85\n",
      "  layers.1.swin3.mlp.fc1      119560     0.85\n",
      "  layers.5.swin3.mlp.fc1      119560     0.85\n",
      "  layers.4.swin3.mlp.fc1      119560     0.85\n",
      "  layers.3.swin3.mlp.fc1      119560     0.85\n",
      "  layers.5.swin3.mlp.fc2      119316     0.84\n",
      "  layers.4.swin3.mlp.fc2      119316     0.84\n",
      "  layers.1.swin3.mlp.fc2      119316     0.84\n",
      "  layers.3.swin3.mlp.fc2      119316     0.84\n",
      "  layers.2.swin3.mlp.fc2      119316     0.84\n",
      "  layers.0.swin3.mlp.fc2      119316     0.84\n",
      "  conv_before_upsample.0      103744     0.73\n",
      " layers.4.swin1.attn.qkv       97740     0.69\n",
      " layers.0.swin1.attn.qkv       97740     0.69\n",
      " layers.2.swin1.attn.qkv       97740     0.69\n",
      " layers.3.swin1.attn.qkv       97740     0.69\n",
      " layers.1.swin1.attn.qkv       97740     0.69\n",
      " layers.5.swin1.attn.qkv       97740     0.69\n",
      "  layers.5.swin5.mlp.fc2       95172     0.67\n",
      "  layers.3.swin5.mlp.fc1       95172     0.67\n",
      "layers.2.swin5.attn.proj       95172     0.67\n",
      "  layers.1.swin5.mlp.fc2       95172     0.67\n",
      "  layers.2.swin5.mlp.fc1       95172     0.67\n",
      "layers.5.swin5.attn.proj       95172     0.67\n",
      "layers.3.swin5.attn.proj       95172     0.67\n",
      "layers.1.swin5.attn.proj       95172     0.67\n",
      "  layers.5.swin5.mlp.fc1       95172     0.67\n",
      "  layers.1.swin5.mlp.fc1       95172     0.67\n",
      "  layers.2.swin5.mlp.fc2       95172     0.67\n",
      "  layers.3.swin5.mlp.fc2       95172     0.67\n",
      "layers.4.swin5.attn.proj       95172     0.67\n",
      "  layers.4.swin5.mlp.fc2       95172     0.67\n",
      "layers.0.swin5.attn.proj       95172     0.67\n",
      "  layers.0.swin5.mlp.fc2       95172     0.67\n",
      "  layers.4.swin5.mlp.fc1       95172     0.67\n",
      "  layers.0.swin5.mlp.fc1       95172     0.67\n",
      "  layers.4.swin2.mlp.fc1       90312     0.64\n",
      "  layers.2.swin2.mlp.fc1       90312     0.64\n",
      "  layers.5.swin2.mlp.fc1       90312     0.64\n",
      "  layers.0.swin2.mlp.fc1       90312     0.64\n",
      "  layers.3.swin2.mlp.fc1       90312     0.64\n",
      "  layers.1.swin2.mlp.fc1       90312     0.64\n",
      "  layers.5.swin2.mlp.fc2       90100     0.64\n",
      "  layers.0.swin2.mlp.fc2       90100     0.64\n",
      "  layers.3.swin2.mlp.fc2       90100     0.64\n",
      "  layers.1.swin2.mlp.fc2       90100     0.64\n",
      "  layers.4.swin2.mlp.fc2       90100     0.64\n",
      "  layers.2.swin2.mlp.fc2       90100     0.64\n",
      "  layers.4.swin4.mlp.fc2       76452     0.54\n",
      "  layers.5.swin4.mlp.fc1       76452     0.54\n",
      "layers.4.swin4.attn.proj       76452     0.54\n",
      "  layers.4.swin4.mlp.fc1       76452     0.54\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def layer_param_breakdown(model, topk=50):\n",
    "    rows = []\n",
    "    for name, p in model.named_parameters():\n",
    "        rows.append({\n",
    "            \"param_name\": name,\n",
    "            \"module\": name.rsplit(\".\", 1)[0] if \".\" in name else name,\n",
    "            \"shape\": list(p.shape),\n",
    "            \"num_params\": p.numel(),\n",
    "            \"trainable\": p.requires_grad\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # module 단위로 합치기\n",
    "    by_module = (df.groupby(\"module\", as_index=False)[\"num_params\"]\n",
    "                   .sum()\n",
    "                   .sort_values(\"num_params\", ascending=False))\n",
    "    total = by_module[\"num_params\"].sum()\n",
    "    by_module[\"percent\"] = (by_module[\"num_params\"] / total * 100).round(2)\n",
    "\n",
    "    print(f\"Total params: {total:,}\")\n",
    "    print(by_module.head(topk).to_string(index=False))\n",
    "    return by_module, df\n",
    "\n",
    "by_module, by_param = layer_param_breakdown(model, topk=80)\n",
    "\n",
    "answer = input(\"파일명 suffix를 입력 (예: _v1, _small) : \").strip()\n",
    "suffix = answer if answer else \"\"\n",
    "by_module.to_csv(f\"param_by_module{suffix}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "by_param.to_csv(f\"param_by_param{suffix}.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f66af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 5\n",
      "CUDA device name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00fb985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/ming/SR24/SR24/hat/archs/DRCT_arch.py:106: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
      "/raid/ming/SR24/SR24/hat/archs/DRCT_arch.py:182: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
      "/raid/ming/SR24/SR24/hat/archs/DRCT_arch.py:122: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
      "/raid/ming/SR24/SR24/hat/archs/DRCT_arch.py:195: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 3.81 GiB total capacity; 2.59 GiB already allocated; 8.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 计算 FLOPs\u001b[39;00m\n\u001b[1;32m     14\u001b[0m flop_count \u001b[38;5;241m=\u001b[39m FlopCountAnalysis(model, input_tensor)\n\u001b[0;32m---> 15\u001b[0m flops \u001b[38;5;241m=\u001b[39m \u001b[43mflop_count\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 计算参数量\u001b[39;00m\n\u001b[1;32m     18\u001b[0m params_table \u001b[38;5;241m=\u001b[39m parameter_count_table(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py:248\u001b[0m, in \u001b[0;36mJitModelAnalysis.total\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Returns the total aggregated statistic across all operators\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    for the requested module.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m        int : The aggregated statistic.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     module_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical_module_name(module_name)\n\u001b[1;32m    250\u001b[0m     total_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(stats\u001b[38;5;241m.\u001b[39mcounts[module_name]\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py:551\u001b[0m, in \u001b[0;36mJitModelAnalysis._analyze\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_trace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_tracer_warning\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    550\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mTracerWarning)\n\u001b[0;32m--> 551\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_get_scoped_trace_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aliases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# Assures even modules not in the trace graph are initialized to zero count\u001b[39;00m\n\u001b[1;32m    554\u001b[0m counts \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py:176\u001b[0m, in \u001b[0;36m_get_scoped_trace_graph\u001b[0;34m(module, inputs, aliases)\u001b[0m\n\u001b[1;32m    173\u001b[0m     name \u001b[38;5;241m=\u001b[39m aliases[mod]\n\u001b[1;32m    174\u001b[0m     register_hooks(mod, name)\n\u001b[0;32m--> 176\u001b[0m graph, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m hook_handles:\n\u001b[1;32m    179\u001b[0m     handle\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/jit/_trace.py:1166\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1165\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m-> 1166\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/nn/modules/module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1128\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/nn/modules/module.py:1098\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1098\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/raid/ming/SR24/SR24/hat/archs/DRCT_arch.py:846\u001b[0m, in \u001b[0;36mDRCT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsampler \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixelshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# for classical SR\u001b[39;00m\n\u001b[1;32m    845\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_first(x)\n\u001b[0;32m--> 846\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_after_body(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    847\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_before_upsample(x)\n\u001b[1;32m    848\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_last(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(x))\n",
      "File \u001b[0;32m/raid/ming/SR24/SR24/hat/archs/DRCT_arch.py:832\u001b[0m, in \u001b[0;36mDRCT.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    829\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 832\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)  \u001b[38;5;66;03m# b seq_len c\u001b[39;00m\n\u001b[1;32m    835\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_unembed(x, x_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/nn/modules/module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1128\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/nn/modules/module.py:1098\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1098\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/raid/ming/SR24/SR24/hat/archs/DRCT_arch.py:299\u001b[0m, in \u001b[0;36mSwinBasedFeatFusionBlock.forward\u001b[0;34m(self, x, xsize)\u001b[0m\n\u001b[1;32m    297\u001b[0m         x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjust2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpue(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswin2(torch\u001b[38;5;241m.\u001b[39mcat((x, x1), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), xsize), xsize))))\n\u001b[1;32m    298\u001b[0m         x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjust3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpue(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswin3(torch\u001b[38;5;241m.\u001b[39mcat((x, x1, x2), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), xsize), xsize))))\n\u001b[0;32m--> 299\u001b[0m         x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjust4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpue(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswin4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxsize\u001b[49m\u001b[43m)\u001b[49m, xsize))))\n\u001b[1;32m    300\u001b[0m         x5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjust5(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpue(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswin5(torch\u001b[38;5;241m.\u001b[39mcat((x, x1, x2, x3, x4), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), xsize), xsize)))\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m#                 return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/nn/modules/module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1128\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/nn/modules/module.py:1098\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1098\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/raid/ming/SR24/SR24/hat/archs/DRCT_arch.py:401\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[0;34m(self, x, x_size)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_resolution \u001b[38;5;241m==\u001b[39m x_size:\n\u001b[0;32m--> 401\u001b[0m     attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m     attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x_windows, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_mask(x_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/nn/modules/module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1128\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/hat/lib/python3.8/site-packages/torch/nn/modules/module.py:1098\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1098\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/raid/ming/SR24/SR24/hat/archs/DRCT_arch.py:186\u001b[0m, in \u001b[0;36mWindowAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    183\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# make torchscript happy (cannot use tensor as tuple)\u001b[39;00m\n\u001b[1;32m    185\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m--> 186\u001b[0m attn \u001b[38;5;241m=\u001b[39m (\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    188\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_position_bias_table[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_position_index\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Wh*Ww,Wh*Ww,nH\u001b[39;00m\n\u001b[1;32m    190\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m relative_position_bias\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()  \u001b[38;5;66;03m# nH, Wh*Ww, Wh*Ww\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 3.81 GiB total capacity; 2.59 GiB already allocated; 8.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "# 加载模型\n",
    "# 定义输入大小\n",
    "input_tensor = torch.randn(1, 3, 64, 64).cuda()  # 将输入张量移动到 GPU\n",
    "\n",
    "# 将模型移动到 GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# 计算 FLOPs\n",
    "flop_count = FlopCountAnalysis(model, input_tensor)\n",
    "flops = flop_count.total()\n",
    "\n",
    "# 计算参数量\n",
    "params_table = parameter_count_table(model)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 计算 Multi-Adds 和前向传播次数\n",
    "multi_adds = flops / 2\n",
    "forward_passes = 1\n",
    "\n",
    "# 输出结果\n",
    "print(params_table)\n",
    "print(f\"Params: {params}\")\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Multi-Adds: {multi_adds}\")\n",
    "print(f\"Forward Passes: {forward_passes}\")\n",
    "\n",
    "# 释放 GPU 内存\n",
    "del input_tensor, model\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drct-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
